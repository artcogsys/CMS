# Cognitive Modeling Suite (CMS)

Framework for neural network modeling and analysis of cognitive processes.
The aim of this framework is to provide a generic approach to training
and testing of neural networks on supervised, unsupervised and reinforcement
learning tasks. It will support life-long learning and the use of multiple
agents at the same time.

For specific resource intensive applications it is adviced to develop
optimized code. Still, snippets from CMS can be of use in that context.


This framework is based on chainer (www.chainer.org).

## Tools

Tools.py contains custom analysis tools that can be applied to neural data
generated by natural and artificial brains. Note that much is provided by
python toolboxes like scikit-learn.

## Agent

This package contains agents that can learn to optimize their parameters. They
are defined by a model and an optimizer. They must implement a *run* function
which implements an update for one minibatch of data. We implement
stateful and stateless agents. They latter uses backpropagation through
time.

#### Supervised

Implements supervised agents

#### Reinforcement

Implements reinforcement learning agents

#### Unsupervised

Implements unsupervised agents (TO DO)

## Brain

This package contains everything necessary to build an agent's brain.

#### Links

Custom links that extend Chainer's capabilities.

#### Models

Models take a predictor (i.e. neural network). They translate the output
of a neural network into usable form. If a model is called then it returns
the loss. The *predict* function allows use of the neural network for making
predictions.

#### Monitor

A monitor is used to monitor the internal states of either a model or a
predictor. It is also used to monitor the past states of an RL agent
to allow RL updating. It can store from 1 up to inf number of observations

One or more monitors can be added to an agent, model or predictor. If added to an
agent then it will add automatically to model and predictor. We allow
multiple monitors that each fulfills a specific function

Types of monitors:
* Monitor (generic)
* Oscilloscope (plots one or more variables within a plot window)

#### Networks

Networks are predictors that take input and produce output.

## Examples

Contains examples of how to use CMS

## Sandbox

Work in progress (will be removed in the future)

## World

Contains the machinery to link an agent to a stream of data as implemented by
an iterator. This defines a *world*.

#### Data

Iterators that produce a stream of data. This also takes care of batch
processing. Also contains custom datasets in chainer form that can be
read by data iterators

#### Tasks

Implements various cognitive tasks. Tasks are distinguished from data
in the sense that they produce a data stream as implemented by the
task. Moreover, they can be modulated by agent actions that modify the
state of the task at hand.


## FAQ

How do I test a new network?

* Just write down a model as in networks.py and run it like one of the examples

How do I check particular network states?

* Either examine a stored snapshot or run a snapshot on some test data while monitoring a state. See examples

How do I perform an analysis as the network is learning?

* See test_learning_analysis.py

How do I deal with missing targets?

* Chainer handles missing targets automatically via the value -1. For floats
there is currently no support.

How do I handle trial-based data?

* The SequentialIterator can be used to process a subset of different trials in each epoch.
The reset_state at the onset of each epoch ensures proper resetting. This requires
n_batches to be set to the (fixed) number of time points of which each trial consists.
Also requires that the data is organised as a concatenation of trials.

How do I handle multiple inputs?

* CMS has support for providing a list of datasets as input to a model.

How do I plot intermediate results other than the loss?

* Either define a monitor and set a monitor function (see test_hydranet example)
or do it post hoc on snapshots of the model (see learning analysis example)

## TO DO

* Add multiple reinforcement learning algorithms
* Add streaming data handling video/audio/linguistic
* Add Universe integration
* Allow networks to run without input and/or output
* Add experiments (fully worked examples
* Add unit testing
* Abstract as much as possible to enable use of other libraries
* Add support for multiple agents in life-long learning setting
* Generate full documentation on the fly using Sphinx/Read the Docs
* Move basic datasets to version testing; keep core codebase as clean
as possible, e.g. by moving part of this code to specific test cases
* Create agents that have train and test modes; but how to best support multiagent.
Agents running on environments
* World should be able to return losses
* Allow input/output/anything visualizer during training; not just loss
  Create generic monitor function; allow multiple monitors that also take
  care of snapshots, loss, throughput; define standard monitors; this
  makes comparison harder; since then each agent gets its own figure window
* Think about best integration between CMS and DRM
* Test if we can run multiple RL agents in comparison mode
* fix learning_analysis movie generation
* Allow multiple inheritance for networks so we dont need to set monitor each time
* finish base class loss plot; run test_foo; cleanup base wrt validation part
* why does loss increase for RLagent? Make REINFORCE non actor-critic
* Implement ProbabilisticCategorizationTask
* Implement AAL
* Use chainer's Reporter object to replace Monitor?
* Allow iterators to add to monitor; update this across CMS
* Confusion matrix should operate on integer representations
* Do we use the predict function of models?
* Can we make this predict function return a selected state instead of a softmax output?
* Add terminal state?
* Why does loss not go down for RL agent?
* Add other RL algorithms
* Compare RL efficacy
* Can we get rid of epochs alltogether? Simplifies World code
* Test probabilistic categorization and compare with NRL

